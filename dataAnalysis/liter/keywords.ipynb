{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/asa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/asa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/asa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/asa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/asa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/asa/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/asa/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/asa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "custom_stopwords = set(['older', 'people', 'adults', 'like', 'make', 'help', 'kentucky', 'members', 'use', 'users', 'user', 'world', 'aging', 'using', 'seniors', 'senior', 'age', 'product', 'products', 'new', 'year', 'years', '000', 'provide', 'provides', 'based', 'team', 'way', 'we', 'are', 'we_are', 'nan', 'nan_nan', 'tc', 'pineway', 'medly', 'enables', 'ehr', 'northwest', 'without', 'balloon', 'face', 'Other', 'stakeholder', 'dml', 'which_allows', 'qura', '._', 'africa', 'will_allow', 'cisco', '\\'ll', 'ranging_from', 'avizia', 'whose_mission', '--', 'ehrs', 'zorpia', 'hellocare', '//care.coach', 'karie', 'byenbye', 'cost_to', 'binata', 'inc.', 'etc', '2_)', '1_)', '\\'_s', '80%_%', '20%_%', 'those_who', 'llc', 'we_offer', 'inclue', 'others', 'list', 'top', 'yet','canada','usa','u', 'i_am','we_\\'re', 'it__','american' ,'do_n\\'t','amazon','company__','ceo','vpexam','not_only','nih','california','such_as','our_goal','world__','let','america','\\'s','per','enexor','we_have','we__e','we_believe','we_aim','do_not','allows_us','uk','much','startup','three','more_than','\\'ve','per_year','their_own','able','along_with','elderly','firm','do','kind','does_not','can_be','past','possible','louisville','ii','well','put','also','tennessee','good','fee','have_been','via','associated_with','relevant','overall','black_swift','focus_on','method','don__','was_founded','has_been','as_well','there_are','will_be','by_providing','new_york','due','google','united_states','school','n\\'t','please', 'complete']) \n",
    "sw = stop_words | custom_stopwords\n",
    "punctuation = re.compile(r'[^\\w\\s]')  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(word, tag):\n",
    "    tag_dict = {\"J\": nltk.corpus.wordnet.ADJ,\n",
    "                \"N\": nltk.corpus.wordnet.NOUN,\n",
    "                \"V\": nltk.corpus.wordnet.VERB,\n",
    "                \"R\": nltk.corpus.wordnet.ADV}\n",
    "    return tag_dict.get(tag[0].upper(), nltk.corpus.wordnet.NOUN)\n",
    "\n",
    "# Preprocessing Function\n",
    "def preprocess(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token, tag)) for token, tag in tagged_tokens if token not in sw and not punctuation.search(token) and not token.isdigit() and len(token) > 1]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "df1 = pd.read_excel('/Users/asa/VScode/liter/liter_data/liter1.xls')\n",
    "df2 = pd.read_excel('/Users/asa/VScode/liter/liter_data/liter2.xls')\n",
    "df3 = pd.read_excel('/Users/asa/VScode/liter/liter_data/liter3.xls')\n",
    "df4 = pd.read_excel('/Users/asa/VScode/liter/liter_data/liter4.xls')\n",
    "df5 = pd.read_excel('/Users/asa/VScode/liter/liter_data/liter5.xls')\n",
    "df6 = pd.read_excel('/Users/asa/VScode/liter/liter_data/liter6.xls')\n",
    "\n",
    "\n",
    "df1['text'] = df1[['Article Title','Author Keywords','Abstract']].astype(str).agg(' '.join, axis=1)\n",
    "df2['text'] = df2[['Article Title','Author Keywords','Abstract']].astype(str).agg(' '.join, axis=1)\n",
    "df3['text'] = df3[['Article Title','Author Keywords','Abstract']].astype(str).agg(' '.join, axis=1)\n",
    "df4['text'] = df4[['Article Title','Author Keywords','Abstract']].astype(str).agg(' '.join, axis=1)\n",
    "df5['text'] = df5[['Article Title','Author Keywords','Abstract']].astype(str).agg(' '.join, axis=1)\n",
    "df6['text'] = df6[['Article Title','Author Keywords','Abstract']].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "df1['title'] = df1['Article Title']  \n",
    "df1['affliations'] = df1['Affiliations']  \n",
    "df2['title'] = df2['Article Title']  \n",
    "df2['affliations'] = df2['Affiliations'] \n",
    "df3['title'] = df3['Article Title']  \n",
    "df3['affliations'] = df3['Affiliations'] \n",
    "df4['title'] = df4['Article Title']  \n",
    "df4['affliations'] = df4['Affiliations'] \n",
    "df5['title'] = df5['Article Title']  \n",
    "df5['affliations'] = df5['Affiliations']\n",
    "df6['title'] = df6['Article Title']  \n",
    "df6['affliations'] = df6['Affiliations'] \n",
    "\n",
    "temp_df1 = df1[['text', 'title','affliations']]\n",
    "temp_df2 = df2[['text', 'title','affliations']]\n",
    "temp_df3 = df3[['text', 'title','affliations']]\n",
    "temp_df4 = df4[['text', 'title','affliations']]\n",
    "temp_df5 = df5[['text', 'title','affliations']]\n",
    "temp_df6 = df6[['text', 'title','affliations']]\n",
    "\n",
    "combined_df = pd.concat([temp_df1, temp_df2,temp_df3,temp_df4,temp_df5,temp_df6], ignore_index=True)\n",
    "\n",
    "# combined_df.dropna(subset=['text'], inplace=True)# Cleaning and preprocessing text data\n",
    "# combined_df['text'] = combined_df['text'].replace(r'\\n', ' ', regex=True).replace('nan', '', regex=True)\n",
    "# combined_df['tokens'] = combined_df['text'].apply(preprocess)\n",
    "\n",
    "# # Filtering based on token count\n",
    "# filtered_df = combined_df[combined_df['tokens'].apply(len) >= 5]\n",
    "# print(filtered_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/glove_kmeans/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/miniconda3/envs/glove_kmeans/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/miniconda3/envs/glove_kmeans/lib/python3.8/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/miniconda3/envs/glove_kmeans/lib/python3.8/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Apply all patterns to the text\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns:\n\u001b[0;32m---> 30\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIGNORECASE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Update the 'text' column with the cleaned text\u001b[39;00m\n\u001b[1;32m     33\u001b[0m combined_df\u001b[38;5;241m.\u001b[39mat[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m text\n",
      "File \u001b[0;32m/opt/miniconda3/envs/glove_kmeans/lib/python3.8/re.py:210\u001b[0m, in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msub\u001b[39m(pattern, repl, string, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    204\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "kw_model = KeyBERT(model=model)\n",
    "\n",
    "# Define patterns to remove\n",
    "patterns = [\n",
    "    r'\\belderly\\w*',     # Matches any word starting with \"elderly\"\n",
    "    r'\\bsenior\\w*',     # Matches any word starting with \"senior\"\n",
    "    r'\\bag\\w*',  \n",
    "    r'\\bolder\\w*', \n",
    "    r'\\belderly people\\b', \n",
    "    r'\\bnan\\b',  # Ensure 'nan' is a complete word\n",
    "    r'\\baged persons\\b', \n",
    "    r'\\bage\\b', \n",
    "    r'\\baged people\\b', \n",
    "]\n",
    "\n",
    "# Apply regex substitutions to remove specified patterns\n",
    "for i, row in combined_df.iterrows():\n",
    "    text = row['text']\n",
    "    # Apply all patterns to the text\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Update the 'text' column with the cleaned text\n",
    "    combined_df.at[i, 'text'] = text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = KeyphraseCountVectorizer()\n",
    "\n",
    "def extract_and_print_keywords(text):\n",
    "    # Tokenize the input text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # Convert list of tokens back to a single string if not empty\n",
    "    text_for_kw_extraction = ' '.join(tokens) if tokens else ''\n",
    "\n",
    "    if text_for_kw_extraction.strip():\n",
    "        keywords = kw_model.extract_keywords(text, vectorizer=vectorizer, stop_words='english', use_mmr=True, top_n=5)\n",
    "        if keywords:\n",
    "            print(f\"Keywords for this text: {', '.join([kw[0] for kw in keywords])}\")\n",
    "            return [kw[0] for kw in keywords]\n",
    "        else:\n",
    "            print(\"No keywords extracted.\")\n",
    "            return []\n",
    "    else:\n",
    "        print(\"Skipping empty or invalid text.\")\n",
    "        return []\n",
    "\n",
    "combined_df['keywords'] = combined_df['text'].apply(extract_and_print_keywords)\n",
    "\n",
    "# Function to remove 'nan' from keywords lists\n",
    "def remove_nans(keywords_list):\n",
    "    return [kw for kw in keywords_list if str(kw).lower() != 'nan']\n",
    "\n",
    "# Apply the function to clean 'nan' values from the keywords lists\n",
    "combined_df['keywords'] = combined_df['keywords'].apply(remove_nans)\n",
    "\n",
    "# Filter rows to keep only those with at least 5 keywords\n",
    "filtered_df = combined_df[combined_df['keywords'].apply(len) >= 5]\n",
    "\n",
    "# Save the updated DataFrame\n",
    "filtered_df.to_csv('filtered_keywords.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "def get_keyword_embeddings(keywords):\n",
    "    keyword_embeddings = []\n",
    "    for keyword in tqdm(keywords, desc=\"Computing keyword embeddings\"):\n",
    "        tokenized_keyword = tokenizer.encode(keyword, add_special_tokens=True)\n",
    "        with torch.no_grad():  # Temporarily disable gradients\n",
    "            keyword_embedding = model(torch.tensor([tokenized_keyword]))[0].mean(dim=1).squeeze()\n",
    "        keyword_embeddings.append(keyword_embedding.numpy())  # Convert the detached tensor to a NumPy array\n",
    "    return np.array(keyword_embeddings)\n",
    "\n",
    "filtered_df['keyword_embeddings'] = filtered_df['keywords'].apply(lambda x: get_keyword_embeddings(x))\n",
    "# Filter out rows where 'keyword_embeddings' is an empty list or None\n",
    "filtered_df = filtered_df[filtered_df['keyword_embeddings'].apply(lambda x: x is not None and len(x) > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_df['keyword_embeddings'][1].shape)\n",
    "print(filtered_df['keyword_embeddings'].shape)\n",
    "\n",
    "\n",
    "def concatenate_vectors(embeddings):\n",
    "    # Concatenate the vectors along the last axis (axis=-1)\n",
    "    return np.concatenate(embeddings, axis=-1)\n",
    "\n",
    "# Apply the function to concatenate vectors for each row\n",
    "filtered_df['concatenated_keyword_vectors'] = filtered_df['keyword_embeddings'].apply(concatenate_vectors)\n",
    "print(filtered_df['concatenated_keyword_vectors'][100].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_non_3840_vectors(dataframe, column_name):\n",
    "    # 存储不是3840维的行索引\n",
    "    non_3840_indices = []\n",
    "    \n",
    "    # 遍历DataFrame中的每一行\n",
    "    for index, row in dataframe.iterrows():\n",
    "        vector = row[column_name]\n",
    "        if len(vector) != 3840:\n",
    "            non_3840_indices.append(index)\n",
    "    \n",
    "    # 返回不符合维度的行的索引和数量\n",
    "    return non_3840_indices, len(non_3840_indices)\n",
    "\n",
    "# 假设 filtered_df 是你的 DataFrame，并且 'concatenated_keyword_vectors' 是包含向量的列名\n",
    "non_3840_indices, non_3840_count = find_non_3840_vectors(filtered_df, 'concatenated_keyword_vectors')\n",
    "\n",
    "print(\"不是3840维的向量的行索引:\", non_3840_indices)\n",
    "print(\"不是3840维的向量的数量:\", non_3840_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(filtered_df['keyword_embeddings'][1].shape)\n",
    "\n",
    "def average_vectors(embeddings):\n",
    "    # Compute the mean of the vectors along the first axis (axis=0)\n",
    "    # This assumes that 'embeddings' is a list of vectors for each keyword\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "# Apply the function to average vectors for each row\n",
    "filtered_df['averaged_keyword_vectors'] = filtered_df['keyword_embeddings'].apply(average_vectors)\n",
    "print(filtered_df['averaged_keyword_vectors'][100].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "vectors = np.stack(filtered_df['averaged_keyword_vectors'])\n",
    "\n",
    "# 数据标准化，确保每个特征的平均值为0，方差为1\n",
    "scaler = StandardScaler()\n",
    "vectors_scaled = scaler.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(vectors_scaled)\n",
    "cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Plotting the cumulative explained variance to find the optimal number of components\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(cumulative_explained_variance, marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance by PCA Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Decide on a threshold for cumulative explained variance, e.g., 0.95\n",
    "n_components = np.where(cumulative_explained_variance >= 0.95)[0][0] + 1\n",
    "print(f'Number of components to retain: {n_components}')\n",
    "\n",
    "# Initialize PCA with the number of components found\n",
    "pca = PCA(n_components=n_components)\n",
    "vectors_pca = pca.fit_transform(vectors_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "inertia = []\n",
    "silhouette_scores = []\n",
    "\n",
    "# 定义聚类数的范围\n",
    "range_of_clusters = range(2, 30)\n",
    "\n",
    "for n_clusters in range_of_clusters:\n",
    "    try:\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(vectors_pca)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "        silhouette_scores.append(silhouette_score(vectors_pca, kmeans.labels_))\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred for n_clusters={n_clusters}: {e}\")\n",
    "        continue\n",
    "\n",
    "# 绘制图表\n",
    "def plot_clusters(range_of_clusters, metric_values, metric_name):\n",
    "    plt.plot(range_of_clusters, metric_values, marker='o')\n",
    "    plt.title(f'{metric_name} Method')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel(f'{metric_name} Score')\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_clusters(range_of_clusters, inertia, 'Elbow')\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_clusters(range_of_clusters, silhouette_scores, 'Silhouette')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the number of clusters\n",
    "num_clusters =10  # Adjust based on your needs\n",
    "\n",
    "# Perform K-Means clustering\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(vectors_pca)\n",
    "# Assign the cluster labels to your original DataFrame\n",
    "filtered_df['cluster'] = kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reduce dimensions to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "reduced_vectors = pca.fit_transform(vectors_pca)\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(10, 7))\n",
    "for i in range(num_clusters):\n",
    "    plt.scatter(reduced_vectors[kmeans.labels_ == i, 0], reduced_vectors[kmeans.labels_ == i, 1], label=f'Cluster {i}')\n",
    "plt.legend()\n",
    "plt.title('Cluster Visualization')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'combined_df' is your DataFrame containing the documents\n",
    "# Assign cluster labels to each document based on the cluster assignments\n",
    "filtered_df['cluster_label'] = kmeans.labels_\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "filtered_df.to_csv('label10_cat_pca_liter.csv', index=False)\n",
    "\n",
    "print(\"Cluster labels added and saved to CSV file.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glove_kmeans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
